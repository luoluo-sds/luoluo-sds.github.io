<html>
<head>
<title> Optimization Theory (DATA 620020) </title>
<link rel=stylesheet href="../style.css" type="text/css">
</head>
<body>

<center>
<h1>Optimization Theory (DATA 620020) </h1>

<h3>Spring 2025</h3>
</center>

<h3> Lecture Time and Venue: </h3>
Friday, 18:30-21:05, HGX307 
<br>

<br>

<h3> Reading Materials: </h3>
<ul>

<li> 刘浩洋, 户将, 李勇锋, 文再文. "最优化：建模、算法与理论". 高教出版社, 2020. [<a href="http://faculty.bicmr.pku.edu.cn/~wenzw/optbook.html">link</a>] </li>

<li> 林宙辰, 李欢, 方聪. "机器学习中的加速一阶优化算法". 机械工业出版社, 2021. [<a href="https://e.jd.com/30732153.html">link</a>] </li>

<li> Jorge Nocedal, Stephen J. Wright. "Numerical optimization". Springer, 2006. [<a href="https://link.springer.com/book/10.1007/978-0-387-40065-5">link</a>] </li>

<li> Yurii Nesterov. "Lectures on convex optimization". Springer, 2018. [<a href="https://link.springer.com/book/10.1007/978-3-319-91578-4">link</a>] </li>
  
<li> Ralph Tyrell Rockafellar. "Convex Analysis". Princeton University Press, 1997. [<a href="https://press.princeton.edu/books/paperback/9780691015866/convex-analysis">link</a>] </li>  
  

</ul>
<br>  

<h3> Courseware (subject to changes):  </h3>
<ul>
<li> Course Overview, Optimization for Machine Learning, Review of Linear Algebra. [<a href="data620020/Optimization_Theory_L01.pdf">pdf</a>] </li> 
<li> Matrix Calculus, Topology, Convergence Rates. [<a href="data620020/Optimization_Theory_L02.pdf">pdf</a>] </li> 
<li> Convex Set, Convex Function, Optimal Condition. [<a href="data620020/Optimization_Theory_L03.pdf">pdf</a>] </li> 
<li> Subgradient and Subdifferential, Subdifferential Calculus. [<a href="data620020/Optimization_Theory_L04.pdf">pdf</a>] </li> 
<li> Optimal Condition, Regularity Conditions. [<a href="data620020/Optimization_Theory_L05.pdf">pdf</a>] </li> 
<li> Subgradient and Subdifferential, Subdifferential Calculus. [<a href="data620020/Optimization_Theory_L06.pdf">pdf</a>] </li>   
<li> Black Box Model, Gradient Descent Methods, Polyak–Łojasiewicz Condition. [<a href="data620020/Optimization_Theory_L07.pdf">pdf</a>] </li>     
<li> Line Search Methods, Barzilai-Borwein Step Size, Parameter-Free Methods. [<a href="data620020/Optimization_Theory_L08.pdf">pdf</a>] </li>     
<li> Heavy Ball Methods, Nesterov's Acceleration. [<a href="data620020/Optimization_Theory_L09.pdf">pdf</a>] </li>       
<li> Lower Complexity Bound, Nonsmooth Convex Optimization. [<a href="data620020/Optimization_Theory_L10.pdf">pdf</a>] </li>       
<li> Subgradient Descent Method, Smoothing Technique, Proximal Gradient Methods. [<a href="data620020/Optimization_Theory_L11.pdf">pdf</a>] </li>   
<li> Newton's Method, Damped Newton's Method. [<a href="data620020/Optimization_Theory_L12.pdf">pdf</a>] </li>     
</ul>



<br> <a href="../index.html"> BACK </a>
</body>
</html>


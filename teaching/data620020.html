
<html>
<head>
<title> Optimization Theory (DATA 620020) </title>
<link rel=stylesheet href="../style.css" type="text/css">
</head>
<body>

<center>
<h1>Optimization Theory (DATA 620020) </h1>

<h3>Spring 2023</h3>
</center>

<h3> Lecture Time and Venue: </h3>
Thursday, 18:30-21:05, H4206 
<br>

<br>

<h3> Reading Materials: </h3>
<ul>

<li> 刘浩洋, 户将, 李勇锋, 文再文. "最优化：建模、算法与理论". 高教出版社, 2020. [<a href="http://faculty.bicmr.pku.edu.cn/~wenzw/optbook.html">link</a>] </li>

<li> 林宙辰, 李欢, 方聪. "机器学习中的加速一阶优化算法". 机械工业出版社, 2021. [<a href="https://e.jd.com/30732153.html">link</a>] </li>

<li> Jorge Nocedal, Stephen J. Wright. "Numerical optimization". Springer, 2006. [<a href="https://link.springer.com/book/10.1007/978-0-387-40065-5">link</a>] </li>

<li> Yurii Nesterov. "Lectures on convex optimization". Springer, 2018. [<a href="https://link.springer.com/book/10.1007/978-3-319-91578-4">link</a>] </li>
  
<li> Ralph Tyrell Rockafellar. "Convex Analysis". Princeton University Press, 1997. [<a href="https://press.princeton.edu/books/paperback/9780691015866/convex-analysis">link</a>] </li>  
  

</ul>
<br>  

<h3> Courseware (subject to changes):  </h3>
<ul>
<li> Feb. 23: Course Overview, Optimization for Machine Learning, Topology. [<a href="data620020/Optimization_Theory_L01.pdf">pdf</a>] </li>
<li> Mar. 02: Convex Set, Convex Function, Subgradient, Subdifferential. [<a href="data620020/Optimization_Theory_L02.pdf">pdf</a>] </li>
<li> Mar. 09: Subdifferential Calculus, Lipschitz Continuity, Smoothness, Strong Convexity. [<a href="data620020/Optimization_Theory_L03.pdf">pdf</a>] </li>  
<li> Mar. 16: Second-Order Characterization, Black Box Model, Gradient Decsent Method. [<a href="data620020/Optimization_Theory_L04.pdf">pdf</a>] </li>   
<li> Mar. 23: Gradient Decsent Method, Polyak–&#321;ojasiewicz Condition, Line Search Method. [<a href="data620020/Optimization_Theory_L05.pdf">pdf</a>] </li>   
<li> Mar. 30: Polyak's Heavy Ball Method, Nesterov's Acceleration. [<a href="data620020/Optimization_Theory_L06.pdf">pdf</a>] </li>   
<li> Apr. 06: Lower Complexity Bounds, Nonsmooth Convex Optimizaiton. [<a href="data620020/Optimization_Theory_L07.pdf">pdf</a>] </li>     
<li> Apr. 13: Subgradient Method, Smoothing, Proximal Gradient Descent. [<a href="data620020/Optimization_Theory_L08.pdf">pdf</a>] </li>     
<li> Apr. 20: Newton's Method, Damped Newton Method. [<a href="data620020/Optimization_Theory_L09.pdf">pdf</a>] </li>   
<li> Apr. 27: Self-Concordant Functions, Global Convergence Analysis. [<a href="data620020/Optimization_Theory_L10.pdf">pdf</a>] </li> 
<li> May. 04: (Limited-Memory) Classical Quasi-Newton Methods. [<a href="data620020/Optimization_Theory_L11.pdf">pdf</a>] </li> 
<li> May. 11: Greedy/Randomized Quasi-Newton Methods, Block Quasi-Newton Methods. [<a href="data620020/Optimization_Theory_L12.pdf">pdf</a>] </li>  
<li> May. 18: Stochastic Gradient Descent, Variance Reduction. [<a href="data620020/Optimization_Theory_L13.pdf">pdf</a>] </li> 
<li> May. 25: Stochastic Variance Reduced Gradient, Catalyst Acceleration, Katyusha. [<a href="data620020/Optimization_Theory_L14.pdf">pdf</a>] </li> 
<li> Jun. 01: Stochastic Recursive Gradient, Zeroth-Order Optimization. [<a href="data620020/Optimization_Theory_L15.pdf">pdf</a>] </li> 
<li> Jun. 01: Zeroth-Order Optimization, Distributed Optimization. [<a href="data620020/Optimization_Theory_L15.pdf">pdf</a>] </li>   
</ul>



<br> <a href="../index.html"> BACK </a>
</body>
</html>

